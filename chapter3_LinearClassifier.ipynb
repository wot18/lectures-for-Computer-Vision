{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision(Machine Vision) for Oversea Phd Candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content included in this chapter can depicted by the following figure:\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-1.png'> </center>\n",
    "\n",
    "<center> Fig.3-1 The content of this chapter. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Image $X$\n",
    "\n",
    "The dataset used in the following sections is CIFAR-10, which contains 60000 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "CIFAR10 Dataset: Consists of 50,000 training samples and 10,000 test samples divided into ten classes: airplane, car, bird, cat, deer, dog, frog, horse, ship, truck. These images are color images with size 32*32.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-2.jpg'> </center>\n",
    "\n",
    "<center> Fig.3-2 The dataset of CIFAR-10. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There 3 types of image, namely RGB, grayscale and binary image. The following figure depicts the difference of these 3 types of image.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-3.jpg'> </center>\n",
    "\n",
    "<center> Fig.3-3 The difference of RGB, grayscale and binary image. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary image is a kind of image, which only contains 0 and 1, as shown in the following figure.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-4.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-4 The binary image. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grayscale image is a kind of image, which only contains 0-255, as shown in the following figure.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-5.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-5 The grayscale image. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB image is a kind of image, which contains 3 channels, namely R, G and B, as shown in the following figure.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-6.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-6 The RGB image. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No matter what kind of the input image, most classification methods are based on the input feature vector.** Therefore, we usually need to find some methods to convert 2D image into 1D feature vector. The simplest method is to use the pixel value as the feature vector, as shown in the following figure.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-7.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-7 From 2D image to 1D feature vector. </center>\n",
    "\n",
    "However, the pixel value is not a good feature vector, because the pixel value is not robust to the image rotation, translation, scaling and so on. Therefore, we need to find some robust features to represent the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** In CIFAR10, how many dimensions does each image convert to a vector?\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\question3-1.jpg' width=\"600\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "\n",
    "Three reasons for using linear classifiers as the foundation in selecting classification models:\n",
    "\n",
    "1. Because of its simple form and easy to understand.\n",
    "\n",
    "2. Through hierarchical structures (neural networks) or high-dimensional mappings (support vector machines), they can form powerful nonlinear models.\n",
    "\n",
    "3. Their computational efficiency compared to more complex models.\n",
    "\n",
    "**What is a linear classifier?**\n",
    "\n",
    "A linear classifier is a linear mapping that maps input image features to class scores.\n",
    "\n",
    "\n",
    "\n",
    "**Linear Classifier Decision**\n",
    "\n",
    "For each class $i$, there is a corresponding linear classifier with its own parameters $w_i$ and $b_i$. The decision function for the $i^{th}$ class is defined as:\n",
    "$$f_i(x, w_i) = w_i^T x + b_i,$$\n",
    "where $x$ represents the input $d$-dimensional image vector, $c$ is the number of classes, and:\n",
    "* $w_i = [w_{i1} \\; ... \\; w_{id}]^T$ is the weight vector for the $i^{th}$ class,\n",
    "* $b_i$ is the bias term.\n",
    "\n",
    "There is a separate decision function for each class, where $i=1,\\ldots,c$.\n",
    "\n",
    "**Decision Rule:**\n",
    "The rule for deciding the class of an input image $x$ is as follows:\n",
    "If $f_i(x) > f_j(x)$, $\\forall j \\neq i$, then the input image $x$ belongs to the $i^{th}$ class.\n",
    "\n",
    "This means that if the score assigned by the $i^{th}$ classifier is higher than the scores assigned by all other classifiers, the image is classified into the $i^{th}$ class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig. 3-8 shows an example of a linear classifier being used to classify images into one of three categories: cars, cats, or birds. A picture of a cat playing with a toy ball is presented as an input to the linear classifier. The classifier assigns a score to each category based on the image features using the formula:\n",
    "$$w_i^T x + b_i = f_i, \\quad i = 1,2,3$$\n",
    "\n",
    "Here, $w_i$ represents the weights associated with the $i^{th}$ category, $x$ is the input image, $b_i$ is the bias term, and $f_i$ is the score assigned to the $i^{th}$ category. The classifier decides the category of the image based on the highest score among the three categories (cars, cats, and birds). In this case, since the image shows a cat, the linear classifier should output the highest score for the 'cat' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig.3-8 shows an example of a linear classifier being used to classify images into one of three categories: cars, cats, or birds. A picture of a cat playing with a toy ball is presented as an input to the linear classifier. The classifier assigns a score to each category based on the image features using the formula:\n",
    "$$w_i^T x + b_i = f_i, \\quad i = 1,2,3$$\n",
    "\n",
    "Here, $w_i$ represents the weights associated with the $i^{th}$ category, $x$ is the input image, $b_i$ is the bias term, and $f_i$ is the score assigned to the $i^{th}$ category. The classifier decides the category of the image based on the highest score among the three categories (cars, cats, and birds). In this case, since the image shows a cat, the linear classifier should output the highest score for the 'cat' category.\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\fig3-8.jpg\" width=\"600\"></center>\n",
    "\n",
    "<center> Fig.3-8 Example of a linear classifier being used to classify images. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to represent the image as a vector. We can do this by stacking the pixel values of the image into a single column vector, as shown in Fig.3-9.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-9.jpg' width=\"300\"> </center>\n",
    "\n",
    "<center> Fig.3-6 Represent the image as a vector. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to calculate the score for each class. The score is calculated by taking the dot product of the weight vector and the feature vector. The class with the highest score is the predicted class.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-10.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-10 Calculate the score for each class. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class of the image is decided by the highest score.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-11.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-11 Decide the image class according to the highest score. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Representation of Linear Classifier:**\n",
    "\n",
    "Given an input image $x$ with dimensionality $d$, the linear classifier computes a vector of scores $f$ whose length equals the number of classes $c$. This process can be represented mathematically as:\n",
    "$f(x, W) = Wx + b $\n",
    "where:\n",
    "* $W$ is the weight matrix,\n",
    "* $w_i = [w_{i1} \\; \\cdots \\; w_{id}]^T$ is the weight vector for the $i^{th}$ class,\n",
    "* $b = [b_1 \\; \\cdots \\; b_c]^T$ is the bias vector, and $b_i$ is the bias for the $i^{th}$ class.\n",
    "\n",
    "The matrix multiplication $Wx$ combines the input image with the weight matrix, while the addition of the bias vector $b$ accounts for the offset in the classification scores. This formulation allows the linear classifier to compute multiple scores simultaneously for different classes.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-12.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-12 Matrix representation of linear classifier. </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What are the dimensions of W, x, and b for the CIFAR-10 dataset classification task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** For the CIFAR-10 dataset, which has 10 classes and images of size 32x32x3, we have:\n",
    "$x$ is the image vector, with dimension 3072;\n",
    "$W$ is the weight matrix, with dimension 10x3072;\n",
    "$b$ is the bias vector, with dimension 10x1;\n",
    "$f$ is the score vector, with dimension 10x1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $ i $-th class, the linear classifier is defined as:\n",
    "$$f_i(x, w_i) = w_i^T x + b_i $$\n",
    "where,\n",
    "- $ x $ represents the input $ d $-dimensional image vector.\n",
    "- $ c $ is the number of classes.\n",
    "\n",
    "Weight Vector and Bias:\n",
    "\n",
    "- $ w_i = [w_{i1}, \\ldots, w_{id}]^T $ is the weight vector for the $ i $-th class.\n",
    "\n",
    "- $ b_i $ is the bias term.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "1. **Linear Classifier**: For each class $ i $, there is an associated linear function $ f_i(x, w_i) $ that calculates the probability or score of the input vector $ x $ belonging to that particular class.\n",
    "\n",
    "2. **Weight Vector $ w_i $**: This is a $ d $-dimensional vector representing the importance weights of each dimension in the feature space. Each element $ w_{ij} $ corresponds to the weight of the $ j $-th feature in the input vector $ x $.\n",
    "\n",
    "3. **Bias Term $ b_i $**: A constant term used to adjust the position of the decision boundary.\n",
    "\n",
    "4. **$ x $**: This is the $ d $-dimensional image vector, which represents a data point or sample.\n",
    "\n",
    "5. **$ c $**: The number of classes, indicating how many different categories are to be classified.\n",
    "\n",
    "Using these parameters, the linear classifier can compute the probabilities or scores for each class given the input vector $ x $, thereby enabling multi-class classification tasks. The decision on which class the input vector belongs to is made based on these computed values, often by choosing the class with the highest score.\n",
    "\n",
    "The weight vectore $ w_i $ can be resize to the same dimension as the input image, therefore each weight vector can be treated as a template of the corresponding class. The higher classifier score, the more similar the input image is to the template of corresponding class.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-13.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-13 Each weight vectore can be treated as a template. </center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The decision boundary for linear classifiers**\n",
    "\n",
    "When we convert an input image into a feature vector, what we have done is a mapping from the image space to the feature space, as shown in Fig.3-14. \n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-14.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-14 Mapping from image space to feature space. </center>\n",
    "\n",
    "The decision boundary is the hyperplane that separates the feature space into regions corresponding to different classes. The hyperplane is defined by the equation $ w_i^T x + b_i = 0 $. \n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-15.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-15 The decision boundaries. </center>\n",
    "\n",
    "The weight vector $ w_i $ is perpendicular to the decision boundary which direction is decided by $w_i$, and the bias term $ b_i $ determines the position of the hyperplane, or the offset of the hyperplane.The decision boundary is the hyperplane that separates the feature space into regions corresponding to different classes. In Fig.3-16, - The direction of the arrow represents the positive direction of the classifier. The further away from the decision surface along the arrow direction, the higher the score.\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-16.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-16 The decision boundaries. </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "To provide an optimal classification model, in addition to the model itself, we also need loss functions and optimization algorithms for assistance.\n",
    "\n",
    "In summary, creating an effective classification model involves not only designing the model but also utilizing appropriate loss functions and optimization algorithms to enhance its performance. These components work together to ensure that the model can accurately classify data and improve over time through training.\n",
    "\n",
    "In Fig.3-17 and Fig.3-18 we have two classifiers, the problem is how to measure the effect of the classifier on the current sample?\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-17.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-17 The first classifier. </center>\n",
    "\n",
    "<center> <img src='.\\\\figures\\\\fig3-18.jpg' width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-18 The second classifier. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example samples, which classifier has better classification results: Classifier 1 or Classifier 2? How can this be quantitatively expressed? Loss functions are needed for help.\n",
    "\n",
    "Loss functions bridge the gap between model performance and model parameters, guiding parameter optimization.\n",
    "\n",
    "- A loss function is a function used to measure the inconsistency between the predictions of a given classifier and the true values. Its output is typically a non-negative real number.\n",
    "\n",
    "- The non-negative real value output can serve as feedback to adjust the classifier's parameters, reducing the current instance's corresponding loss value and improving the classifier's classification effect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General definition of the loss function:\n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_{i=1}^{N} L_i(f(x_i; W), y_i) $$\n",
    "\n",
    "Where:\n",
    "- $ x_i $ represents the ith picture in the dataset;\n",
    "- $ f(x_i; W) $ is the classifier's prediction for classifying $ x_i $;\n",
    "- $ y_i $ is the true label of sample i (integer);\n",
    "- $ L_i $ is the loss value of the ith sample;\n",
    "- $ L $ is the dataset loss, which is the average of all sample losses in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiclass support vector machine (SVM) loss function** also known as hinge loss function, is a commonly used loss function in SVMs. It is defined as follows:\n",
    "\n",
    "$$\n",
    "L_i = \\sum_{j \\neq y_i} \n",
    "\\begin{cases} \n",
    "0 & \\text{if } s_{y_i} \\geq s_{ij} + 1 \\\\\n",
    "s_{ij} - s_{y_i} + 1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This can also be written as:\n",
    "\n",
    "$$\n",
    "L_i = \\sum_{j \\neq y_i} \\max(0, s_{ij} - s_{y_i} + 1)\n",
    "$$\n",
    "\n",
    "In this context, $L_i$ represents the loss associated with the ith sample, $s_{y_i}$ denotes the score assigned to the correct class $y_i$, and $s_{ij}$ represents the score assigned to any other class $j$ different from $y_i$. The max function ensures that only scores violating the margin constraint contribute to the total loss.\n",
    "\n",
    "The definition of $s_{ij}$ is:\n",
    "\n",
    "$$\n",
    "s_{ij} = f_j(x_i, w_j, b_j) = w_j^T x_i + b_j\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ j $ —— category label, taking values within the range {1, 2, ..., c};\n",
    "- $ w_j, b_j $ —— parameters of the jth category classifier;\n",
    "- $ x_i $ —— representing the ith sample in the dataset;\n",
    "- $ s_{ij} $ —— predictive score of the jth category for the ith sample;\n",
    "- $ s_{yi} $ —— predictive score of the true category for the ith sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-calss SVM loss function indicate that If the score of the correct category is higher than the score of the incorrect categories by at least one point, there will be no loss. Otherwise, there will be a loss.\n",
    "\n",
    "The $max(0,·)$ loss is often referred to as hinge loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume there are three training samples, each belonging to a different category, and the classifier is a linear classifier represented by $f(x,W)=Wx+b$, where the weight matrix $W$ and bias term $b$ are known. The scores assigned by the classifier to the three samples are as follows:\n",
    "\n",
    "The loss of current classifier for the bird image is:\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\question3-2-1.jpg\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss of current classifier for the cat image is:\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\question3-2-2.jpg\" width=\"600\"></center>\n",
    "\n",
    "The loss of current classifier for the car image is:\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\question3-2-3.jpg\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss of current classifier for the whole image dataset is:\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^N L_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summary, the loss function can be written as:\n",
    "$$\n",
    "L=\\frac{1}{N} \\sum_{i} L_{i}\\left(f\\left(x_{i}, W\\right), y_{i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiclass SVM loss function for a single sample can be written as:\n",
    "$$\n",
    "L_{i}=\\sum_{j \\neq y_{i}} \\max \\left(0, s_{i j}-s_{i y_{i}}+1\\right)\n",
    "$$\n",
    "where $s_{i j}$ is the score assigned to the jth category for the ith sample, $s_{i y_{i}}$ is the score assigned to the true category for the ith sample, and $y_{i}$ is the true category label for the ith sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of $s_{ij}$ is:\n",
    "$$\n",
    "s_{ij} = f_j(x_i, w_j, b_j) = w_j^T x_i + b_j\n",
    "$$\n",
    "where:\n",
    "- $ j $ —— category label, taking values within the range {1, 2, ..., c};\n",
    "- $ w_j, b_j $ —— parameters of the jth category classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. What is the maximum/minimum value of multi-class support vector machine loss $ L_i $?\n",
    "\n",
    "2. If $w$ and $b$ are very small during initialization, what will be the loss $ L $?\n",
    "\n",
    "3. Considering all categories (including $ j = y_i $), how will the loss $ L_i $ change?\n",
    "\n",
    "4. When calculating the total loss $ L $, if we use sum instead of average?\n",
    "\n",
    "5. If using $ L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1)^2 $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Revisiting loss function**:\n",
    "\n",
    "$$ L(W) = \\frac{1}{N} \\sum_i L_i(f(x_i, W), y_i) $$\n",
    "\n",
    "Assuming there exists a $W$ that makes the loss function $L=0$, is this $W$ unique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Assume two linear classifiers $f_1(x, W_1) = W_1x, f_2(x, W_2) = W_2x$, where $W_2 = 2W_1$. For the following image, given the scoring results of classifier 1 (as shown in the table below), please calculate the scoring results of classifier 2, as well as the multiclass SVM loss for both classifiers on the current sample:\n",
    "\n",
    "<center><img src=\".\\\\figures\\\\fig3-19.jpg\" width=\"600\"><\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that $W$ is not unique, because $W_2$ also has $L = 0$\n",
    "\n",
    "How should one choose between $W_1$ and $W_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce the regularization loss, so that the total loss function can be given as:\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\equation3-1.jpg\" width=\"600\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R(W)$: This is a function that depends on the weight values but not on the input data. It helps control the complexity of the model.\n",
    "\n",
    "$λ$: This is a hyperparameter that controls the relative importance of the regularization term compared to the data loss term. A higher value of λ means more emphasis on regularization, which can help prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are Hyperparameters?**\n",
    "Hyperparameters are parameters whose values are set before the learning process begins, rather than being learned during the process. They generally have a significant impact on the performance of the model.\n",
    "\n",
    "When $ \\lambda = 0 $: The optimization result only considers the data loss.\n",
    "\n",
    "When $ \\lambda = \\infty $: The optimization result ignores the data loss and only considers the weight loss. In this case, the system's optimal solution is $ W = 0 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the total loss function is given as:\n",
    "\n",
    "$$ L(W) = \\frac{1}{N} \\sum_{i=1}^N L_i(f(x_i, W), y_i) + \\lambda R(W) $$\n",
    "\n",
    "- **Data Loss**: This part of the equation represents the model's prediction error on the training set. It measures how well the model fits the data.\n",
    "  \n",
    "  $$ \\frac{1}{N} \\sum_{i=1}^N L_i(f(x_i, W), y_i) $$\n",
    "\n",
    "- **Regularization Loss**: This term prevents the model from overfitting to the training data by penalizing large weights.\n",
    "  \n",
    "  $$ \\lambda R(W) $$\n",
    "\n",
    "where $R(W)$ can be formulated as **L2 Regularization Term** which can be expressed as:\n",
    "\n",
    "$$ R(W) = \\sum_k \\sum_l W_{k,l}^2 $$\n",
    "\n",
    "This formula calculates the sum of the squares of all elements in the weight matrix $W$. Each element $ W_{k,l} $ represents a weight connecting the $k$-th input feature to the $l$-th output neuron.\n",
    "\n",
    "Explanation: L2 regularization aims to penalize large weight values. By adding the squared magnitude of each weight to the loss function, it encourages the model to distribute its weights more evenly across all features. This helps prevent the model from relying too heavily on a small number of features, promoting better generalization to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L2 Loss Example**\n",
    "\n",
    "Sample Input: $ x = [1, 1, 1, 1] $\n",
    "\n",
    "Classifier1: $ W_1 = [1, 0, 0, 0] $\n",
    "\n",
    "Classifier2: $ W_2 = [0.25, 0.25, 0.25, 0.25] $\n",
    "\n",
    "Classifier Outputs: $ W_1^T x = W_2^T x = 1 $\n",
    "\n",
    "Regularization Loss:\n",
    "$$ R(W_1) = 1 $$\n",
    "$$ R(W_2) = 0.25 $$\n",
    "\n",
    "Conclusion:\n",
    "Classifier 2 has a smaller total loss due to the lower regularization loss. \n",
    "\n",
    "The regularization term introduces a preference in the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These regularization terms are used to prevent overfitting by penalizing large weights in the model.\n",
    "\n",
    "**Regularization Terms**\n",
    "\n",
    "1. **L1 Regularization (Lasso Regression):**\n",
    "   $$\n",
    "   R(W) = \\sum_k \\sum_l |W_{k,l}|\n",
    "   $$\n",
    "   - This term sums the absolute values of all elements in the weight matrix $ W $.\n",
    "\n",
    "2. **L2 Regularization (Ridge Regression):**\n",
    "   $$\n",
    "   R(W) = \\sum_k \\sum_l W_{k,l}^2\n",
    "   $$\n",
    "   - This term sums the squared values of all elements in the weight matrix $ W $.\n",
    "\n",
    "3. **Elastic Net Regularization (Combination of L1 and L2):**\n",
    "   $$\n",
    "   R(W) = \\sum_k \\sum_l (\\beta W_{k,l}^2 + |W_{k,l}|)\n",
    "   $$\n",
    "   - This term combines both L1 and L2 regularization, where $ \\beta $ is a parameter controlling the balance between the two types of regularization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Methods\n",
    "\n",
    "What is Parameter Optimization?\n",
    "\n",
    "Parameter optimization is one of the core steps in machine learning. It uses the output value of the loss function as feedback signals to adjust the classifier parameters, aiming to improve the predictive performance of the classifier on the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of optimization algorithms\n",
    "\n",
    "The objective of optimization algorithms is to find the set of parameters $ W $ that minimizes the loss function $ L $. The loss function $ L $ is defined as follows:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=1}^{N} L_i + \\lambda R(W)\n",
    "$$\n",
    "\n",
    "where $ N $ is the number of samples, $ L_i $ is the loss of the $ i $-th sample, $ \\lambda $ is a regularization parameter, $ R(W) $ is the regularization term.\n",
    "\n",
    "The goal is to find the set of parameters $ W $ that makes the loss function $ L $ reach its minimum. Direct methods involve setting the partial derivative of $ L $ with respect to $ W $ equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = 0\n",
    "$$\n",
    "\n",
    "However, the form of $ L $ is often complex, making it difficult to directly solve for $ W $ from this equality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent Algorithm**\n",
    "\n",
    "A simple yet efficient iterative optimization method!\n",
    "\n",
    "Where to Go?\n",
    "\n",
    "Answer: Negative Gradient Method\n",
    "\n",
    "How Far to Go?\n",
    "\n",
    "Answer: Step Length Determines\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\fig3-20.jpg\" width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-20 The idea of gradient descent algorithm </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of gradient descent can be summarized as follows:\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    # Calculate gradient\n",
    "    gradient <- calculate_gradient(loss, training_samples, current_weights)\n",
    "\n",
    "    # Update weights\n",
    "    updated_weights <- current_weights - learning_rate * gradient\n",
    "\n",
    "    # Check convergence or stopping criteria\n",
    "    if convergence_or_stopping_criteria_met():\n",
    "        break\n",
    "return updated_weights\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- `calculate_gradient(loss, training_samples, current_weights)` computes the gradient of the loss function with respect to the current weights using the provided training samples.\n",
    "- `learning_rate` is a predefined constant that determines the step size at each iteration.\n",
    "- The loop continues until a convergence criterion is met or a maximum number of iterations is reached.\n",
    "- The final updated weights are returned after the loop terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to calculate gradient?**\n",
    "\n",
    "1. Numerical Method\n",
    "   \n",
    "   - Large amount of computation required, not very accurate!\n",
    "   \n",
    "   For a one-dimensional variable, the derivative of the function is calculated as follows:\n",
    "   \n",
    "   $$\n",
    "   \\frac{dL(w)}{dw} = \\lim_{h \\to 0} \\frac{L(w + h) - L(w)}{h}\n",
    "   $$\n",
    "   \n",
    "   Example: For the loss function $ L(w) = w^2 $, find the gradient at point $ w = 1 $.\n",
    "   \n",
    "   $$\n",
    "   \\frac{dL(w)}{dw} = \\lim_{h \\to 0} \\frac{L(w + h) - L(w)}{h} \\approx \\frac{L(1 + 0.0001) - L(1)}{0.0001} = 2.0001\n",
    "   $$\n",
    "\n",
    "2. Analytical Method\n",
    "\n",
    "   - Advantages: Precise, Fast, Easy to derive the derivative function\n",
    "\n",
    "   - Disadvantage: Prone to errors during derivation\n",
    "\n",
    "   Example: Given the loss function $ L(w) = w^2 $, calculate the gradient at $ w = 1 $. Solution:\n",
    "\n",
    "$$ \\nabla L(w) = 2w $$\n",
    "\n",
    "   At $ w = 1 $:\n",
    "   \n",
    "$$ \\nabla_{w=1} L(w) = 2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To summarize, \n",
    "\n",
    "Numerical gradient descent: Approximate, slow, easy to write.\n",
    "\n",
    "Analytic gradient descent: Accurate, fast, prone to errors.\n",
    "\n",
    "What is the role of numerical gradient descent? \n",
    "\n",
    "Answer: When computing the gradient, generally use analytic gradient descent, but numerical gradient descent is mainly used to verify the correctness of the analytic gradient descent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of computational graph is show below:\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\fig3-21.jpg\" width=\"600\"> </center>\n",
    "\n",
    "<center> Fig.3-21 Computational graph </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent Algorithm's Computational Efficiency**\n",
    "\n",
    "**Gradient Descent**: Utilize all samples to calculate the loss and update the gradient.\n",
    "\n",
    "Loss Function:\n",
    "$$ L(W) = \\frac{1}{N} \\sum_{i=1}^{N} L_i(x_i, y_i, W) + \\lambda R(W) $$\n",
    "\n",
    "Gradient of the Loss Function:\n",
    "$$ \\nabla_W L(W) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_W L_i(x_i, y_i, W) + \\lambda \\nabla_W R(W) $$\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    weight_gradient <- compute_gradient(loss, training_samples, weights)\n",
    "    weights <- weights - learning_rate * weight_gradient\n",
    "```\n",
    "\n",
    "When $N$ is very large, the computational cost of calculating the gradient of the weights is high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent Algorithm**\n",
    "\n",
    "**Stochastic Gradient Descent**: Randomly select one sample $ x_i $, calculate the loss, and update the gradient.\n",
    "\n",
    "Loss Function:\n",
    "$$ L(W) = L_i(x_i, y_i, W) + \\lambda R(W) $$\n",
    "\n",
    "Gradient of the Loss Function:\n",
    "$$ \\nabla_W L(W) = \\nabla_W L_i(x_i, y_i, W) + \\lambda \\nabla_W R(W) $$\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    data <- sample from training_data (training_data, 1)\n",
    "    weight_gradient <- compute_gradient(loss, data, weights)\n",
    "    weights <- weights - learning_rate * weight_gradient\n",
    "```\n",
    "\n",
    "Training with a single sample may introduce a lot of noise, not necessarily moving towards the overall optimal direction in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-Batch Gradient Descent**\n",
    "\n",
    "**Mini-batch gradient descent**: Select m samples randomly at each iteration, calculate the loss and update the gradient.\n",
    "\n",
    "Loss Function:\n",
    "$$ L(W) = \\frac{1}{m} \\sum_{i=1}^{m} L_i(x_i, y_i, W) + \\lambda R(W) $$\n",
    "\n",
    "Gradient of the Loss Function:\n",
    "$$ \\nabla_W L(W) = \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_W L_i(x_i, y_i, W) + \\lambda \\nabla_W R(W) $$\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    data <- sample from training data (training_data, batch_size)\n",
    "    weight_gradient <- compute_gradient(loss, data, weights)\n",
    "    weights <- weights - learning_rate * weight_gradient\n",
    "```\n",
    "\n",
    "- **Iteration**: Represents the number of iterations.\n",
    "- **Batch size**: The amount of data used per iteration.\n",
    "- **Epoch**: One epoch represents going through all the training data once.\n",
    "\n",
    "**Tip**: Typically use powers of 2 as the batch size, such as selecting 32 or 64 or 128 samples at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Summary\n",
    "\n",
    "Gradient Descent Method\n",
    "```python\n",
    "while True:\n",
    "    weight_gradient <- compute_gradient(loss, training_data, weights)\n",
    "    weights <- weights - learning_rate * weight_gradient\n",
    "```\n",
    "  - Weight gradient: Calculate the gradient using the entire training dataset.\n",
    "  - Update weights by subtracting the product of the learning rate and the weight gradient.\n",
    "\n",
    "Stochastic Gradient Descent Method\n",
    "```python\n",
    "while True:\n",
    "    data <- sample_from_training_data(training_data, 1)\n",
    "    weight_gradient <- compute_gradient(loss, data, weights)\n",
    "    weights <- weights - learning_rate * weight_gradient\n",
    "```\n",
    "  - Data: Sample a single piece of data from the training dataset.\n",
    "  - Calculate the gradient using the sampled data.\n",
    "  - Update weights by subtracting the product of the learning rate and the weight gradient.\n",
    "\n",
    "Mini-Batch Gradient Descent Method\n",
    "```python\n",
    "while True:\n",
    "    mini_batch_data <- sample_from_training_data(training_data, batch_size)\n",
    "    weight_gradient <- compute_gradient(loss, mini_batch_data, weights)\n",
    "    weights <- weights - learning_rate * weight_gradient\n",
    "```\n",
    "  - Mini-batch data: Sample a subset of the training dataset based on the batch size.\n",
    "  - Calculate the gradient using the mini-batch data.\n",
    "  - Update weights by subtracting the product of the learning rate and the weight gradient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset splitting\n",
    "\n",
    "If a given dataset is splitted into training and test datasets, the test dataset is used to evaluate the performance of the model. But the question is if the model contains hyperparameters (such as regularization strength), how can we find the best hyperparameters for generalization ability? The answer is use the validation set.\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\fig3-22.jpg\" width=\"600\"> </center>\n",
    "\n",
    "<center> Fig. 3-22 Dataset splitting </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Set, Validation Set, and Test Set\n",
    "\n",
    "* Training Set: Used for training the model with given hyperparameters.\n",
    "\n",
    "* Validation Set: Used for choosing the best hyperparameters.\n",
    "\n",
    "* Test Set: Used for evaluating the generalization ability of the model.\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\fig3-23.jpg\" width=\"600\"> </center>\n",
    "\n",
    "<center> Fig. 3-23 Traning set, validation set and test set </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Fold Cross-Validation**\n",
    "\n",
    "Problem: When there is little data, it is possible that the validation set contains too few samples, making it unable to statistically represent the data.\n",
    "\n",
    "This problem is easily discovered: if different random shuffles are performed before dividing the data, the resulting models' performance differences will be significant, indicating the existence of this issue.\n",
    "\n",
    "Next, two methods will be introduced to solve this problem: K-fold cross-validation and repeated K-fold cross-validation. These are two approaches to address this issue effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-Fold Cross-Validation: This method ensures that every part of the data is used for both training and validation, providing a more robust estimate of the model's performance. Take 3-fold cross-validation as an example. The data is divided into three parts, and each part is used as the validation set while the other two parts are used as the training set. The average performance of the three models is taken as the final performance.\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\fig3-24.jpg\" width=\"600\"> </center>\n",
    "\n",
    "<center> Fig. 3-24 3-Fold Cross-Validation </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Repeated K-Fold Cross-Validation with Shuffled Data: This technique involves randomly shuffling the data before each repetition of K-Fold Cross-Validation to ensure that the datasets in each fold are different. This can further enhance the reliability of model evaluation and reduce bias.\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\fig3-25.jpg\" width=\"600\"> </center>\n",
    "\n",
    "<center> Fig. 3-25 Repeated K-Fold Cross-Validation with Shuffled Data </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Can data be directly used? What are the processing methods?\n",
    "\n",
    "Data preprocessing is an essential step in preparing raw data for analysis or machine learning tasks. It involves several techniques to clean, transform, and normalize the data to improve its quality and make it suitable for modeling. By applying these preprocessing steps, the data becomes cleaner, more consistent, and better suited for downstream analyses or predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean Removal: In data preprocessing, mean removal (also known as centering or zero-mean normalization) is a common technique for standardizing data. This process involves subtracting the mean of each feature (column) from each observation (row) within that feature. The goal is to transform each feature into a new distribution with a mean of zero, which can help certain machine learning algorithms converge more effectively and improve model performance.\n",
    "\n",
    "Specifically, for a feature $X$, the mean-removed version $X'$ can be calculated using the following formula:\n",
    "\n",
    "$$ X' = X - \\mu $$\n",
    "\n",
    "Where:\n",
    "- $X$ is the original feature vector.\n",
    "- $\\mu$ is the mean of $X$ (i.e., the average of all samples for that feature).\n",
    "\n",
    "* Mean removal is part of feature scaling and is often combined with variance normalization, where the data is scaled to have unit variance. When both are applied together, this process is referred to as standardization, which makes the data follow a standard normal distribution, i.e., with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "The standardization formula is as follows:\n",
    "\n",
    "$$ X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma} $$\n",
    "\n",
    "Where:\n",
    "- $X_{\\text{standardized}}$ is the standardized feature.\n",
    "- $\\sigma$ is the standard deviation of $X$.\n",
    "\n",
    "Mean removal and standardization are particularly useful for algorithms that are sensitive to the scale of input data, such as linear regression, logistic regression, support vector machines (SVMs), and neural networks. These algorithms typically expect input data to be centered around zero and to have similar scales, which facilitates the optimization process and helps in finding the optimal solution more efficiently.\n",
    "\n",
    "<center> <img src=\".\\\\figures\\fig3-26.jpg\" width=\"600\"> </center>\n",
    "\n",
    "<center> Fig. 3-26 Mean removal and Standardization</center>\n",
    "\n",
    "Here's an example of how to perform mean removal on a dataset using NumPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Mean:\n",
      " [4. 5. 6.]\n",
      "Mean-removed data:\n",
      " [[-3. -3. -3.]\n",
      " [ 0.  0.  0.]\n",
      " [ 3.  3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Calculate the mean of each feature\n",
    "mean = np.mean(X, axis=0)\n",
    "\n",
    "# Mean removal\n",
    "X_mean_removed = X - mean\n",
    "\n",
    "print(\"Original data:\\n\", X)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Mean-removed data:\\n\", X_mean_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decorrelation in Data Preprocessing**\n",
    "\n",
    "In data preprocessing, decorrelation refers to the process of transforming the data to reduce or eliminate linear dependencies among features. When features are highly correlated, this can lead to issues during model training, such as multicollinearity, which can make parameter estimates in certain regression algorithms unstable. Additionally, removing correlations between features can simplify models, improve interpretability, and enhance generalization.\n",
    "\n",
    "Common methods for decorrelation include:\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**:\n",
    "   - PCA is a statistical technique that transforms a set of possibly correlated variables into a set of linearly uncorrelated variables, known as principal components.\n",
    "   - The principal components are a rotated version of the original data, where the first principal component has the largest variance, and each subsequent component is orthogonal to all preceding ones and has the next highest variance.\n",
    "   - By selecting the top few principal components, one can reduce the dimensionality of the data while retaining most of the information.\n",
    "\n",
    "2. **Independent Component Analysis (ICA)**:\n",
    "   - ICA aims to find a linear transformation such that the transformed components are as statistically independent as possible.\n",
    "   - Unlike PCA, ICA considers not only non-correlation but also the statistical independence between components.\n",
    "   - ICA is commonly used in signal separation problems, such as blind source separation.\n",
    "\n",
    "3. **Whitening**:\n",
    "   - Whitening is a special type of decorrelation technique that not only makes the data uncorrelated but also scales each component to have unit variance.\n",
    "   - After whitening, the data will have a spherical distribution, which is particularly useful for machine learning algorithms that assume specific distributions of the input data, such as certain types of neural networks.\n",
    "\n",
    "4. **Transformations Based on the Covariance Matrix**:\n",
    "   - By calculating the covariance matrix of the data and performing an eigenvalue decomposition, one can select the corresponding eigenvectors as the transformation basis, thereby achieving decorrelation.\n",
    "   - This method is similar to PCA but does not necessarily involve dimensionality reduction.\n",
    "\n",
    "Decorrelation operations are very helpful for improving the quality of datasets, simplifying models, and enhancing the performance of machine learning models. However, it is important to ensure that no significant information is lost during the decorrelation process and to consider the interpretability requirements of the actual application.\n",
    "\n",
    "<center> <img src=\".\\\\figures\\\\fig3-27.jpg\" width=\"600\"> </center>\n",
    "<center> Fig. 3-27 Decorrelation operations </center>\n",
    "\n",
    "Here is an example of how to perform PCA on a dataset using Python's scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n",
      "Transformed data:\n",
      " [[ 4.24264069 -0.        ]\n",
      " [ 1.41421356  0.        ]\n",
      " [-1.41421356  0.        ]\n",
      " [-4.24264069  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming X is your dataset with shape (n_samples, n_features)\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # Specify the number of components you want to keep\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Original data:\\n\", X)\n",
    "print(\"Transformed data:\\n\", X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `n_components=2` specifies that we want to keep 2 principal components. You can adjust this value based on your specific requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
